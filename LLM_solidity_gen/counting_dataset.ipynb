{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d66783d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5258fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(contents, path):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(contents)\n",
    "    \n",
    "def read_from_file(path):\n",
    "    with open(path) as f:\n",
    "        return ''.join(f.readlines())\n",
    "    \n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll-1\n",
    "    return -9999, -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb2447a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_path = [f for f in os.listdir('prompts_folder/new_function_dataset_builder/codes_simplified') if f.endswith('sol')]\n",
    "len(codes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed83365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_path = [f for f in os.listdir('prompts_folder/new_function_dataset_builder/codes') if f.endswith('sol')]\n",
    "len(codes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3649c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for p in codes_path:\n",
    "    p = 'prompts_folder/new_function_dataset_builder/codes/' + p\n",
    "    lengths.append(len(read_from_file(p).split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed8c8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304.575"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e4386e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f94eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e41a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d123f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f219fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\section{Evaluation}\n",
    "% We select DAO that use full proposal when submit to the governance contract, we select . 3161 proposal from DAOstack, 2310 proposals from DAOhaus, 1746 proposals from Compound\\_based, 15443 proposals from Aragon.\n",
    "In the evaluation section, we aim to answer the following three research questions.\n",
    "\n",
    "To begin with, we want to evaluate the effectiveness of our proposed framework as a whole. This can be done from two aspects. First, we can measure the effectiveness by comparing the end results (the final codes after all rounds of updates) of our method to the implementations of the functions in the original contract codes; this shows how close to the groundtruth codes our generated codes are. Second, we compare our final codes to the initial codes generated by a simple GPT-4 agent that did not involve any enhancements yet; this shows the continuous improvements and corrections that our framework injected to the initially generated codes, which distinguishes our result from the straightforward generation capabilities of GPT-4.. \n",
    "Overall, the first research question is summarized in the following as RQ1:\n",
    "\n",
    "\\begin{tcolorbox}[size=title]\n",
    "\\textbf{RQ1:} How does our feedback system improve the quality of generation, compared to traditional ChatGPT-based approaches?\n",
    "\\end{tcolorbox}\n",
    "\n",
    "\\jinan{Some descriptions of RQ2 goes here}\n",
    "\n",
    "\\begin{tcolorbox}[size=title]\n",
    "\\textbf{RQ2:} What is the performance of each type of agent? How does each types of agent affect the evolution and correction of the generated codes? \n",
    "%\\todo{compare the outputs of each agent, and see how the codes are improved round by round, and see which agent contributed to the most improvement}\n",
    "\n",
    "%\\todo{contribution: by examining the effect of each agent individually, we get to reveal the effect of each type of way that attemps to improve the quality of code generation by GPT}\n",
    "\\end{tcolorbox}\n",
    "\n",
    "\n",
    "\\jinan{Some descriptions of RQ3 goes here}\n",
    "\\begin{tcolorbox}[size=title]\n",
    "\\textbf{RQ3:} How effective is our gas optimization system? \n",
    "%\\todo{e.g. construct a small dataset to test how much of the gas inefficient patterns it could find, as well as examples from the original papers. Also, could report how many gas inefficient patterns are found in the generated codes, which reveals the fundamental capabilities of GPT to find gas inefficient patterns}\n",
    "\\end{tcolorbox}\n",
    "\n",
    "\n",
    "\n",
    "\\subsection{Code Generation Task Dataset}\n",
    "In this section, we describe the steps that we took to establish a new dataset called \\textbf{SolGenEval} (\\textbf{S}olidity \\textbf{G}eneration \\textbf{E}valuation) that aims to evaluate the code generation framework that we proposed. Overall, the major steps that we took to construct the dataset are listed below.\n",
    "\n",
    "\n",
    "\\noindent\\textbf{STEP 1: Candidate Contract Selecting} \n",
    "\n",
    "We aim to select candidate contracts from the most frequently called contracts, since a high frequency of usage is a good indicator of code quality as well as range of applicability. To accomplish this, we extracted the top 5 contract addresses called by contracts that implemented the Compound Protocol ecosystem. This ecosystem has the benefit of having covered a very wide range of language features and use cases of Solidity and having been widely adopted in popular contracts. \n",
    "\n",
    "\\textbf{STEP 2: Contract Codes Extending}\n",
    "\n",
    "At this step, we extract the codes of the target deployed contract addresses. Each deployed contract address could contain multiple code files with different contracts performing various functionalities.\n",
    "Overall, we ended up with 40 code files with 304.575 lines of code on average. \n",
    "\n",
    "Note that we did not just take the codes files that contain the target functions that were more frequently called, and rather took all the contract files under the ecosystem of each of the target addresses. \n",
    "The main motivation behind this design choice is that contracts codes obtained from STEP 1 are standalone files from individual projects, which lacks comprehensiveness and completeness. To address that, the added contexual contracts provides code files that constitutes a richer coding dataset.\n",
    "The intuition here is that we want to test the ability of our framework to make generations of various types of functionalities within a Solidity software project. \n",
    "\n",
    "\n",
    "\\todo{(argument of why do this: we manually examined other contracts and found that )}\n",
    "For the following reasons:\n",
    "1. Due to the rate limiting features of the GPT-4 API, it is very difficult to carry out large scale automatic testing that cover a lot of contracts and API calls.\n",
    "2. The selected top 5 contracts, after extending, already covers a lot of different contracts and comprehensive language features of Solidity \\todo{(e.g. discuss how much of the language features are covered, maybe refer to https://solidity-by-example.org/). also make sure to discuss the wide range of use cases}. \n",
    "\n",
    "\n",
    "\n",
    "\\textbf{STEP 3: contract refactoring}\n",
    "At this step, we perform refactoring on these contracts, with the goal of underscoring one function of a given contract at a time. Specifically, this is done by \n",
    "\\todo{maybe put an algorithm section here}\n",
    "\n",
    "The motivation behind this design choice is that we want to test the capability of our framework to complete functions in a given software project. \n",
    "\n",
    "\\todo{fill in here}\n",
    "This step results in .... refactored function code files.\n",
    "\n",
    "\n",
    "Refactoring the contracts into contracts with blanks, with one blank function each.\n",
    "Wrote a description for each missing function\n",
    "\n",
    "The intuition is to test the performance of our framework in the use scenario of completing a missing function, given contexts.\n",
    "\n",
    "\\todo{put a picture comparison of examples from the two datasets, from remix}\n",
    "\n",
    "\\todo{considering if we should do this or not}\n",
    "Note that we make the following optimizations/considerations during this step:\n",
    "1. removed functions that mainly rely on directing function calls to another (proxy functions)\n",
    "2. removed functions that are too short, meaningless, or too simple (typically less than 3 lines) (we want our dataset to be more challenging as GPT-4 is already capable to perform generations on very basic functions, and we target more complex ones.)\n",
    "\n",
    "also removed comments that preceed the function so this does not constitute extra information for the LLM.\n",
    "\n",
    "We ended up with 626 refactored contracts, where each contract contains one function.\n",
    "\n",
    "\n",
    "\\textbf{STEP 4: Simplification and Deduplication}\n",
    "\n",
    "It is worth noting that we have obtained a large number of codes in the previous step, and for the following reasons (copy from the previous section), we want to reduce the number of files that we are to deal with, and to put the spotlight in the most note-worthing code files.\n",
    "\n",
    "\n",
    "1. took the top 5\n",
    "2. deduplication\n",
    "3. removed codes that are too short\n",
    "\n",
    "\n",
    "We perform deduplication at this step.\n",
    "\n",
    "\\todo{note that we also removed functions that have the same function name, even if they might not have the same signature (i.e. parameters), this is because we want to:\n",
    "1. maximally diversify our dataset, as having the same function name but different parameter list still could mean that they have more similar implementations than they would for those that have different names. In other words, we want to use this to maximally avoid the case of our datasets being biased toward a certain type of function }\n",
    "\n",
    "The single-function contracts obtained from the previous step could contain functions that have duplicate names. This is common because multiple different contracts could be employing a similar or same funcionality as one of the intermediate steps, and this could be feflected by calling a function that have the same name.\n",
    "\n",
    "To reduce the redundancy of our dataset, we decided to only keep only one of the refactored one-function contracts that are duplicates. The filtering is done by random selection. This has the following benefits to our dataset:\n",
    "1. more diversified\n",
    "2. less bias and more well balanced (since otherwise the evaluation statistics might be a little skewed towards reflecting single-function contracts that have a higher presence)\n",
    "\n",
    "\\todo{For example, the function ??? appeared ??? times in the original dataset, where this unbalanced presence could skew the testing results}\n",
    "\n",
    "After this round of deduplication, we ended up with 291 single-function contracts, where each contract contains one function.\n",
    "\n",
    "\n",
    "\\textbf{STEP 5: }\n",
    "We created two datasets, each containing a different version of each file, called \\textbf{\\textit{D\\textsubscript{p}}} and \\textbf{\\textit{D\\textsubscript{r}}}\n",
    "\n",
    "\\todo{consider making it a curly d}\n",
    "\n",
    "1. with the function present\n",
    "2. with the function removed\n",
    "\n",
    "This \n",
    "\n",
    "\\textbf{STEP 6: Code File summarization}\n",
    "\n",
    "%\\todo{we try to make our summary at the first step as brief as possible, while preseving the essential information of the codes, with the consideration that we want the most of the generation work to be genuinely done by GPT.}\n",
    "\n",
    "\\todo{contribution: we created a carefully curated dataset which contains code files that cover a wide range of common use cases as well as language features of Solidity. With this, we also attached a set of code summaries for a selected subset of the more complex and frequently used code files from the repository. This has the hope of testing our framework under the generation of more complex cases, since we assume that (cite white paper) GPT is already to make reasonable generations }\n",
    "\n",
    "We summarized important information from each file, including the input format, output format, and a summary of the purpose of the codes. \n",
    "\n",
    "An example is given below: \\todo{put a picture here}\n",
    "\n",
    "The high level idea of the motivation behind this design choice is that \n",
    "\n",
    "\n",
    "\\noindent\\textbf{Discussion:}\n",
    "\n",
    "\\todo{before making SolgenEval public, need to check format issues}\n",
    "\n",
    "1. the SolgenEval could be easily extended following the same steps, and it is not difficult to extend the size of data to the order of tens of thoudsands.\n",
    "\n",
    "2. (Inspired by the \"Evaluating Large Language Models Trained on Code\" paper)\n",
    "\n",
    "3. generalizability:\n",
    "1. The Compound Protocol is a popular protocol that are implemented by many Solidity projects\n",
    "2. The contracts and functionalities that we selected contain rich semantic and syntactic features of Solidity, including modifiers, inheritance, \n",
    "it also contains many features and use cases that are widely seen in Decentralized Applications (DApps). \n",
    "3. also rich use cases\n",
    "\n",
    "\n",
    "Overall, here are some statistics about the dataset that we have constructed, recorded at Table~\\ref{tab:dataset_stats}:\n",
    "\n",
    "\n",
    "\\begin{table}\n",
    "\\centering\n",
    "\\footnotesize\n",
    "\\caption{\\small{Statistics of the dataset}}\n",
    "\\vspace{-2mm}\n",
    "\\begin{tabular}{lc} \n",
    "\\toprule\n",
    "Type        & Number   \\\\\n",
    "\\hline \\hline \n",
    "Number of target contract addresses        &              \\\\\n",
    "Number of contracts          &               \\\\\n",
    "Number of extended code files           &               \\\\ \n",
    "Average number of lines of each code file          &               \\\\\n",
    "\\todo{Put some more rows here}          &               \\\\\n",
    "\n",
    "\n",
    "\\hline \n",
    "todo               &               \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\label{tab:dataset_stats}\n",
    "\\vspace{-2mm}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\\subsection{Comparing to GPT-4 (RQ1)}\n",
    "% \\noindent \\textbf{Motivation.} The efficiency of the sentence classification module plays an important role in extracting the description facts from the code behavior related sentences. We try to evaluate the precision and recall rate of our sentence classification module. \n",
    "\\sectionWritingPlan{JJJ}\n",
    "\n",
    "\\noindent \\textbf{Approach.} \n",
    "\n",
    "% What types of API calls are needed and their timeline\n",
    "\n",
    "core idea: compare the final codes to the original ones by manual verification\n",
    "\n",
    "The authors of this paper read the modifications made to the original codes, examined each of the changes, and made corrections to inappropriate or incorrect lines of change where necessary. \n",
    "\n",
    "How to evaluate the generated codes?\n",
    "1. By manual reading, and report the percentage of codes that have:\n",
    "Incorrect, incomplete, and imprecise generated functions\n",
    "2. By running the compiler as well as some defect-finding tools, report the average number of issues in the generated codes\n",
    "3. Check how the gas optimization works (note: need to make sure that the gas optimization does not )\n",
    "Report the average number of gas issues per document\n",
    "Report the running gas before and after\n",
    "\n",
    "we could also present similar figures to the RQ2\n",
    "\n",
    "we could also report a categorization of the modifications that are made to the codes.\n",
    "\n",
    "\n",
    "\n",
    "\\noindent \\textbf{Result.} \n",
    "\n",
    "\n",
    "\\noindent \\textbf{Answer to RQ1.} \n",
    "\n",
    "\\todo{put some preliminary discussion here as well}\n",
    "\\begin{tcolorbox}[\n",
    "    standard jigsaw,\n",
    "    opacityback=0,  % this works only in combination with the key \"standard jigsaw\"\n",
    "]\n",
    " Finding 1: After a round of careful reading and analysis, we find that ... (describe how the final codes are similar and dissimilar to the original ones. If dissimilar, explain any potential reasons)\n",
    "\\end{tcolorbox}\n",
    "\\vspace{-1mm}\n",
    "\n",
    "\n",
    "\\subsection{Effectiveness of each phase of the framework (RQ2)}\n",
    "\n",
    "\\noindent \\textbf{Approach.} \n",
    "We perform a two-fold evaluation on the intermediate outputs of each agent.\n",
    "\n",
    "First, we examine the intermediate output of each agent and report statistics about the effects of each step of our proposed framework.\n",
    "\n",
    "Second, we compare the intermediate results of each agent and see how the codes get evolved over each step. For example, one of our evaluations attempts to figure out how codes changed, at the level of number of added and deleted codes, from ${CODE_{DirGen}}$ to ${CODE_{CoT}}$, from ${CODE_{DirGen}}$ to ${CODE_{FSE}}$, from ${CODE_{CoT}}$ to ${CODE_{FSE}}$, and so on.\n",
    "\n",
    "\\noindent \\textbf{Result.} \n",
    "\n",
    "\\textbf{PART 1. The Chain of Thought (CoT) Feedback Phase: } the number of steps in the step-by-step CoT plan phase are plotted in the CDF graph in Figure~\\ref{fig:cdf_num_steps}. \n",
    "\n",
    "The Cumulative Distribution Function (CDF) plot in Figure~\\ref{fig:cdf_num_steps} presents the distribution of the number of steps across multiple step-by-step plans. The x-axis denotes the number of steps, while the y-axis represents the cumulative probability. The monotonic increase of the curve from 0 to 1 reveals that as the number of steps in a plan increases, so does the cumulative probability. The steepness of the curve signifies the frequency of plans with a specific number of steps, with a steeper curve indicating a higher probability. The curve's approach to 1 on the far right suggests an upper limit on the number of steps in the majority of plans. The CDF plot, therefore, offers valuable insights into the complexity pattern of our planning system.\n",
    "\n",
    "From Figure~\\ref{fig:RQ2_STEP1_COT_CDF_num_steps} as well as the associated data, we can read that less than 64\\% of the plans that were proposed by the planning agent \\todo{agent name}. In addition, the minimum number of steps is 4 and the maximum is 20, which exhibits that the proposed plan has varied complexity.\n",
    "\\todo{we could also show a comparison to the lines of codes of the original codes (hUman written)}\n",
    "In addition, Figure~\\ref{fig:RQ2_STEP1_COT_BAR_num_steps} also shows the bar graph distribution of the number of steps proposed at each step. \n",
    "\n",
    "\\begin{figure}[t]\n",
    "\t\\centering\n",
    "\t\\includegraphics[width=0.5\\textwidth]{figures/RQ2_STEP1_COT_CDF_num_steps.pdf}\n",
    "\t\\vspace{-2ex}\n",
    "\t\\caption{\\small{The percentage of codes that have each specific number of steps}}\n",
    "       \\vspace{-4ex}\n",
    "\t\\label{fig:RQ2_STEP1_COT_CDF_num_steps}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}[t]\n",
    "\t\\centering\n",
    "\t\\includegraphics[width=0.5\\textwidth]{figures/RQ2_STEP1_COT_BAR_num_steps.pdf}\n",
    "\t\\vspace{-2ex}\n",
    "\t\\caption{\\small{No. of steps in the proposed step-by-step plan}}\n",
    "       \\vspace{-4ex}\n",
    "\t\\label{fig:RQ2_STEP1_COT_BAR_num_steps}\n",
    "\\end{figure}\n",
    "\n",
    "\\textbf{PART 2. The Few-Shot Example (FSE) Feedback Phase: } for this part, we count the number of modification suggestions that are proposed by the GPT-4 agents for each code file during the FSE phase. The results are plotted in the CDF graph in Figure~\\ref{fig:RQ2_STEP2_FSE_CDF_num_mods} as well as the bar graph in Figure~\\ref{fig:RQ2_STEP2_FSE_BAR_num_mods}, which follows the same format as in PART 1. \n",
    "\n",
    "\\jinan{some discussion of the figures goes here}\n",
    "\n",
    "\\begin{figure}[t]\n",
    "\t\\centering\n",
    "\t\\includegraphics[width=0.5\\textwidth]{figures/RQ2_STEP2_FSE_BAR_num_mods.pdf}\n",
    "\t\\vspace{-2ex}\n",
    "\t\\caption{\\small{No. of modifications made to the codes}}\n",
    "       \\vspace{-4ex}\n",
    "\t\\label{fig:RQ2_STEP2_FSE_BAR_num_mods}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}[t]\n",
    "\t\\centering\n",
    "\t\\includegraphics[width=0.5\\textwidth]{figures/RQ2_STEP2_FSE_CDF_num_mods.pdf}\n",
    "\t\\vspace{-2ex}\n",
    "\t\\caption{\\small{The percentage of codes that have each specific number of modifications}}\n",
    "       \\vspace{-4ex}\n",
    "\t\\label{fig:RQ2_STEP2_FSE_CDF_num_mods}\n",
    "\\end{figure}\n",
    "\n",
    "\\textbf{PART 3. Related Functions (RF) Feedback: } for this part, we count the number of modification suggestions that are proposed by the GPT-4 agents for each code file during the RF phase. The results are plotted in the CDF graph in Figure~\\ref{fig:RQ2_STEP3_RE_CDF_num_mods} as well as the bar graph in Figure~\\ref{fig:RQ2_STEP3_RE_BAR_num_mods}, which follows the same format as in PART 1. \n",
    "\n",
    "\\jinan{some discussion of the figures goes here}\n",
    "\n",
    "\n",
    "\\begin{figure}[t]\n",
    "\t\\centering\n",
    "\t\\includegraphics[width=0.5\\textwidth]{figures/RQ2_STEP3_RE_BAR_num_mods.pdf}\n",
    "\t\\vspace{-2ex}\n",
    "\t\\caption{\\small{No. of modifications made to the codes}}\n",
    "       \\vspace{-4ex}\n",
    "\t\\label{fig:RQ2_STEP3_RE_BAR_num_mods}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}[t]\n",
    "\t\\centering\n",
    "\t\\includegraphics[width=0.5\\textwidth]{figures/RQ2_STEP3_RE_CDF_num_mods.pdf}\n",
    "\t\\vspace{-2ex}\n",
    "\t\\caption{\\small{The percentage of codes that have each specific number of modifications}}\n",
    "       \\vspace{-4ex}\n",
    "\t\\label{fig:RQ2_STEP3_RE_CDF_num_mods}\n",
    "\\end{figure}\n",
    "\n",
    "Table~\\ref{tab:num_modified_lines}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{table}\n",
    "\\centering\n",
    "\\footnotesize\n",
    "\\caption{\\small{The average number of added, deleted, and total number of modified lines for each code file, for each combination of steps}}\n",
    "\\vspace{-2mm}\n",
    "\\begin{tabular}{c|ccc} \n",
    "\\toprule\n",
    "\\textbf{Step Sequence}        & \\textbf{\\# added lines}  & \\textbf{\\# deleted lines} & \\textbf{\\# modified lines}   \\\\\n",
    "\\hline \\hline \n",
    "STEP 1 $\\rightarrow$ STEP 2         & 0.8679   & 0.5283 & 1.3962          \\\\\n",
    "STEP 1 $\\rightarrow$ STEP 3       & 3.5472      & 2.3774      & 5.9245                   \\\\\n",
    "STEP 1 $\\rightarrow$ STEP 4      &4.3774       &3.1698       & 7.5477                    \\\\\n",
    "STEP 2 $\\rightarrow$ STEP 3      & 3.000      & 2.1887      & 5.1887                    \\\\ \n",
    "STEP 2 $\\rightarrow$ STEP 4      &3.9057       &3.0377       &6.9434                     \\\\\n",
    "STEP 3 $\\rightarrow$ STEP 4      &1.7170       &1.6415       &3.3585                     \\\\\n",
    "\n",
    "\\hline \n",
    "\\textbf{Average Number}  & \\textbf{2.9025}    & \\textbf{2.1572}    & \\textbf{5.0597}             \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\label{tab:num_modified_lines}\n",
    "\\vspace{-2mm}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{figure*}[htp]\n",
    "\\centering\n",
    "\\begin{subfigure}[h]{0.3\\textwidth}\n",
    "    \\centering\n",
    "    \\includegraphics[width=\\textwidth]{figures/RQ2_STEP1_TO_STEP_2.pdf}\n",
    "    \\vspace{-2mm}\n",
    "    \\caption{\\small{DG to CoT}}\n",
    "    \\label{subfig:RQ2_STEP1_TO_STEP_2}\n",
    "\\end{subfigure}\\quad\n",
    "\\begin{subfigure}[h]{0.3\\textwidth}\n",
    "\t\\centering\n",
    "\t\\includegraphics[width=\\textwidth]{figures/RQ2_STEP1_TO_STEP_3.pdf}\n",
    "\t\\vspace{-2mm}\n",
    "    \\caption{\\small{DG to FSE}}\n",
    "    \\label{subfig:RQ2_STEP1_TO_STEP_3}\n",
    "\\end{subfigure}\\quad\n",
    "\\begin{subfigure}[h]{0.3\\textwidth}\n",
    "\t\\centering\n",
    "\t\\includegraphics[width=\\textwidth]{figures/RQ2_STEP1_TO_STEP_4.pdf}\n",
    "\t\\vspace{-2mm}\n",
    "    \\caption{\\small{DG to RF}}\n",
    "    \\label{subfig:RQ2_STEP1_TO_STEP_4}\n",
    "\\end{subfigure}\n",
    "\\vspace{-4mm}\n",
    "\\caption{\\small{The statistics of modified content. }}\n",
    "\\label{fig:NOT_NEEDED_1}\n",
    "\\end{figure*}\n",
    "\n",
    "\\begin{figure*}[htp]\n",
    "\\centering\n",
    "\\begin{subfigure}[h]{0.3\\textwidth}\n",
    "    \\centering\n",
    "    \\includegraphics[width=\\textwidth]{figures/RQ2_STEP2_TO_STEP_3.pdf}\n",
    "    \\vspace{-2mm}\n",
    "    \\caption{\\small{CoT to FSE}}\n",
    "    \\label{subfig:RQ2_STEP2_TO_STEP_3}\n",
    "\\end{subfigure}\\quad\n",
    "\\begin{subfigure}[h]{0.3\\textwidth}\n",
    "\t\\centering\n",
    "\t\\includegraphics[width=\\textwidth]{figures/RQ2_STEP2_TO_STEP_4.pdf}\n",
    "\t\\vspace{-2mm}\n",
    "    \\caption{\\small{CoT to RF}}\n",
    "    \\label{subfig:RQ2_STEP2_TO_STEP_4}\n",
    "\\end{subfigure}\\quad\n",
    "\\begin{subfigure}[h]{0.3\\textwidth}\n",
    "\t\\centering\n",
    "\t\\includegraphics[width=\\textwidth]{figures/RQ2_STEP3_TO_STEP_4.pdf}\n",
    "\t\\vspace{-2mm}\n",
    "    \\caption{\\small{FSE to RF}}\n",
    "    \\label{subfig:RQ2_STEP3_TO_STEP_4}\n",
    "\\end{subfigure}\n",
    "\\vspace{-4mm}\n",
    "\\caption{\\small{The statistics of modified content. }}\n",
    "\\label{fig:NOT_NEEDED_2}\n",
    "\\end{figure*}\n",
    "\n",
    "\n",
    "\n",
    "\\noindent \\textbf{Answer to RQ2.} \n",
    "\n",
    "\\begin{tcolorbox}[\n",
    "    standard jigsaw,\n",
    "    opacityback=0,  % this works only in combination with the key \"standard jigsaw\"\n",
    "]\n",
    " Finding 3: Overall, our DG -> CoT -> FSE -> RF code updating sequence continuously performs improvements on the initially generated codes.\n",
    "\\end{tcolorbox}\n",
    "\\vspace{-1mm}\n",
    "\n",
    "\n",
    "\n",
    "\\subsection{Gas optimization effectiveness (RQ3)}\n",
    "\n",
    " % some statistics that we could report:\n",
    "% 1. the number of gas inefficient cases that are found\n",
    "% 2. the amount of gas that is reduced after running and implementing the gas reduction feedback\n",
    "\n",
    "\\noindent \\textbf{Approach.} \n",
    "To answer the RQ3, we build a set of datasets aiming to examine the effectiveness of the GPT-model to detect gas inefficient patterns.\n",
    "\n",
    "Note: report the number of each type of gas inefficiency in the dataset, and the percentage of discovery of each type.\n",
    "\n",
    "We also use other language models as a comparison (e.g. Claud-Instant, Palm, ChatGPT-3.5), to show how well GPT-4 performs in terms of finding gas inefficient patterns following the templates.\n",
    "(this constitutes a table)\n",
    "\n",
    "In addition, for each report of gas-inefficient case, manually verify it and report the number of false/true cases.\n",
    "\n",
    "\n",
    "%\\todo{Use GPT to generate test cases}\n",
    "\n",
    "\n",
    "\\noindent \\textbf{Result.} \n",
    "\n",
    "The result is shown in Table~\\ref{tab:gas_opt_accuracy}, which shows that the gas optimization is capable of perform detection of inefficient gas patterns at a high accuracy.\n",
    " \n",
    "\\begin{table}\n",
    "\\centering  \n",
    "\\footnotesize\n",
    "\\caption{\\small{Detection accuracy of the gas optimization agent}}\n",
    "\\vspace{-2mm}\n",
    "\\begin{tabular}{cccc}  \n",
    "\\toprule\n",
    "\\textbf{Pattern type}        & \\textbf{Detected}   \\\\\n",
    "\\hline \\hline \n",
    "1. dead code         & \\checkmark              \\\\\n",
    "2. Opaque predicate          & \\checkmark              \\\\\n",
    "3. Expensive operations in a loop            & \\checkmark               \\\\ \n",
    "4. Constant outcome of a loop          & \\checkmark              \\\\\n",
    "5. Loop fusion         & \\checkmark              \\\\\n",
    "6. Repeated computations in a loop         & \\checkmark              \\\\\n",
    "7. Comparison with unilateral outcome in a loop          & \\checkmark              \\\\\n",
    "\n",
    "\\hline \n",
    "\\textbf{Percentage of patterns detected}               & \\textbf{100\\%}              \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\label{tab:gas_opt_accuracy}\n",
    "\\vspace{-2mm}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "\\noindent \\textbf{Answer to RQ3.} \n",
    "\n",
    "\\begin{tcolorbox}[\n",
    "    standard jigsaw,\n",
    "    opacityback=0,  % this works only in combination with the key \"standard jigsaw\"\n",
    "]\n",
    " Finding 4: We find that GPT-4 is capable to match against prescribed patterns at a very high accuracy. Specifically we were able to find ... (add stuff about the findings here)\n",
    "\\end{tcolorbox}\n",
    "\\vspace{-1mm}\n",
    "\n",
    "\n",
    "\\todo{need to modify this}\n",
    "The gas optimization is capable of perform detection of inefficient gas patterns at a high accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "% No.1 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b\n",
    "% ABI for the implementation contract at 0xbafe01ff935c7305907c33bf824352ee5979b526, likely using a custom proxy implementation.\n",
    "% Previously recorded to be on 0x234b619b4f4e405665f7d94f2ce60c24256032b5.\n",
    "% Contract name: Unitroller\n",
    "% Contract at 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b\n",
    "% Proxy name: Comptroller\n",
    "% Proxy at: 0xbafe01ff935c7305907c33bf824352ee5979b526\n",
    "\n",
    "\n",
    "% No.2 '0x0831172b9b136813b0b35e7cc898b1398bb4d7e7'\n",
    "%  ABI for the implementation contract at 0xc6df585f8721bfafbb1580bd4034315696eab9ca, using the EIP-1967 Transparent Proxy pattern.\n",
    "% Previously recorded to be on 0x8e516a4357d217405313e40b18357b82bb460917.\n",
    "% Contract name: ERC1967Proxy\n",
    "% Contract at '0x0831172b9b136813b0b35e7cc898b1398bb4d7e7'\n",
    "% Proxy name: GovernorBravoDelegate\n",
    "% Proxy at: 0xc6df585f8721BfafBb1580bD4034315696EAB9cA\n",
    "\n",
    "\n",
    "% No.3 0x4dCf7407AE5C07f8681e1659f626E114A7667339\n",
    "% ABI for the implementation contract at 0xc6df585f8721bfafbb1580bd4034315696eab9ca, using the EIP-1967 Transparent Proxy pattern.\n",
    "% Previously recorded to be on 0x8e516a4357d217405313e40b18357b82bb460917..\n",
    "% Contract name: Unitroller\n",
    "% Contract at 0x4dCf7407AE5C07f8681e1659f626E114A7667339\n",
    "% Proxy name: Comptroller\n",
    "% Proxy at: 0x48c5e896d241Afd1Aee73ae19259A2e234256A85\n",
    "\n",
    "\n",
    "% No.4 0x41d5d79431a913c4ae7d69a668ecdfe5ff9dfb68\n",
    "% ABI for the implementation contract at 0xbafe01ff935c7305907c33bf824352ee5979b526, likely using a custom proxy implementation.\n",
    "% Previously recorded to be on 0x234b619b4f4e405665f7d94f2ce60c24256032b5.\n",
    "% Contract name: INV\n",
    "% Contract at 0x41d5d79431a913c4ae7d69a668ecdfe5ff9dfb68\n",
    "% Proxy name: N/A\n",
    "% Proxy at: N/A\n",
    "\n",
    "\n",
    "% No.5 0x010caccf546de952c5591b7018340549be2eb641\n",
    "% ABI for the implementation contract at 0xbafe01ff935c7305907c33bf824352ee5979b526, likely using a custom proxy implementation.\n",
    "% Previously recorded to be on 0x234b619b4f4e405665f7d94f2ce60c24256032b5.\n",
    "% Contract name: Pool\n",
    "% Contract at 0x010caccf546de952c5591b7018340549be2eb641\n",
    "% Proxy name: N/A\n",
    "% Proxy at: N/A\n",
    "\n",
    "\n",
    "\n",
    "% The more complete list of the top addresses called by Compound DAOs are given below (contracts that are called more than 30 times):\n",
    "\n",
    "% ('0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b', 202),\n",
    "%  ('0x0831172b9b136813b0b35e7cc898b1398bb4d7e7', 152),\n",
    "%  ('0x4dcf7407ae5c07f8681e1659f626e114a7667339', 149),\n",
    "%  ('0x41d5d79431a913c4ae7d69a668ecdfe5ff9dfb68', 83),\n",
    "%  ('0x010caccf546de952c5591b7018340549be2eb641', 83),\n",
    "%  ('0x0cec1a9154ff802e7934fc916ed7ca50bde6844e', 66),\n",
    "%  ('0xe2e17b2cbbf48211fa7eb8a875360e5e39ba2602', 49),\n",
    "%  ('0x7740792812a00510b50022d84e5c4ac390e01417', 48),\n",
    "%  ('0x97990b693835da58a281636296d2bf02787dea17', 46),\n",
    "%  ('0xde30da39c46104798bb5aa3fe8b9e0e1f348163f', 40),\n",
    "%  ('0x865377367054516e17014ccded1e7d814edc9ce4', 40),\n",
    "%  ('0xe8929afd47064efd36a7fb51da3f8c5eb40c4cb4', 38),\n",
    "%  ('0x8d5ed43dca8c2f7dfb20cf7b53cc7e593635d7b9', 34)\n",
    "\n",
    "\n",
    "\n",
    "% Here are the actual addresses that we processed:\n",
    "\n",
    "\n",
    "% 0x010caccf546de952c5591b7018340549be2eb641\n",
    "% 0x010caccf546de952c5591b7018340549be2eb641.zip\n",
    "% 0x41d5d79431a913c4ae7d69a668ecdfe5ff9dfb68\n",
    "% 0x41d5d79431a913c4ae7d69a668ecdfe5ff9dfb68.zip\n",
    "% 0x48c5e896d241afd1aee73ae19259a2e234256a85\n",
    "% 0x48c5e896d241afd1aee73ae19259a2e234256a85.zip\n",
    "% 0xbafe01ff935c7305907c33bf824352ee5979b526\n",
    "% 0xbafe01ff935c7305907c33bf824352ee5979b526.zip\n",
    "% 0xc6df585f8721bfafbb1580bd4034315696eab9ca\n",
    "% 0xc6df585f8721bfafbb1580bd4034315696eab9ca.zip\n",
    "\n",
    "\n",
    "% Also explain the motivation of why choose the Compound DAO \n",
    "% 1. very popular and mature, well tested, less chance to be buggy\n",
    "% 2. DAOs are gaining more popularity (explain DAO here as well)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
