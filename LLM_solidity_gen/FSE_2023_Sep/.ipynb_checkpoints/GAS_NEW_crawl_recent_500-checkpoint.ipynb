{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cccb3a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinanjiang/miniconda3/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request,urllib.error\n",
    "import urllib.parse\n",
    "import requests\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b73b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bdf8e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(contents, path):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(contents)\n",
    "    \n",
    "def read_from_file(path):\n",
    "    with open(path) as f:\n",
    "        return ''.join(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d63ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8f0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count=0\n",
    "def askURL():#获取当前页的智能合约地址\n",
    "    head={ #模拟浏览器头部信息\n",
    "        \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    baseurl=\"https://etherscan.io/contractsVerified/\" #获取智能合约地址页面 基地址\n",
    "    for i in range(4,21):\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        url=baseurl+str(i)#1-5页 加在基地址后面\n",
    "\n",
    "        t=random.random()*1.0\n",
    "        print(t)\n",
    "        time.sleep(t)\n",
    "\n",
    "        request = urllib.request.Request(url,headers=head,method=\"GET\")#封装访问信息\n",
    "\n",
    "        print(url)\n",
    "\n",
    "        attempts = 0\n",
    "        success = False\n",
    "        while attempts < 5 and not success:\n",
    "            try:\n",
    "                response = urllib.request.urlopen(request, timeout=3.5)  # 访问网页，必须设置 访问时间超过多少，否则会被拒绝访问\n",
    "                print(\"successfully get the page\")\n",
    "                success = True\n",
    "            except:\n",
    "                attempts += 1\n",
    "                if attempts == 5:\n",
    "                    print(\"attempt = 5,fail to get the \",i,\" page.\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "\n",
    "        # try:\n",
    "        #     response = urllib.request.urlopen(request,timeout=3.5)#访问网页，必须设置 访问时间超过多少，否则会被拒绝访问\n",
    "        #     print(\"ok\")\n",
    "        #\n",
    "        # except:\n",
    "        #      i = i-1\n",
    "        #      continue\n",
    "        html=response.read().decode(\"utf-8\")#以gbk的方式解码，添加在列表里\n",
    "        response.close()\n",
    "\n",
    "        # print(html)\n",
    "        Parse_html(html)#在每一个合约中抓取源代码\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Parse_html(html):\n",
    "    bs = BeautifulSoup(html,\"html.parser\")#解析每个html文件，\n",
    "  #  print(bs)\n",
    "    head = {  # 模拟浏览器头部信息，向豆瓣服务器发送消息\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
    "    }\n",
    "  #  t_list=bs.find_all(class_=\"hash-tag text-truncate\") #将每个页面中的25个合约地址获取到\n",
    "   # t_list = bs.find_all(\"a\",{'class':'me-1'})  # 将每个页面中的25个合约地址获取到\n",
    "    t_list = bs.find_all(class_ = 'me-1')\n",
    "   # pid = soup.findAll('a', {'class': 'sister'})\n",
    "    result = []\n",
    "\n",
    "    for i in t_list:\n",
    "         if i.get('href')!= None:\n",
    "               i=i.get('href')  # 对每项使用get函数取得tag属性值\n",
    "               if(i.endswith('code')):\n",
    "                    result.append(i)\n",
    "    print(len(result))\n",
    "\n",
    "    str1=\"https://etherscan.io/\" #合约页面基地址/https://etherscan.io/0x0D18b9252a9Dc77d149930Abc49e3C2Dd7057e81#code\n",
    "    str2=\"#code\"#合约页面地址的最后部分，合约地址在str1、str2中间\n",
    "\n",
    "  #  print(t_list)\n",
    "\n",
    "    for item in result:\n",
    "        global count #全局变量 文件名\n",
    "        url=str1+item #拼全合约地址\n",
    "        request = urllib.request.Request(url, headers=head, method=\"GET\") #打包访问信息\n",
    "        attempts = 0\n",
    "        success = False\n",
    "        while attempts < 3 and not success:\n",
    "            try:\n",
    "                response = urllib.request.urlopen(request,timeout=3.5) #访问合约页面\n",
    "                success = True\n",
    "            except:\n",
    "                attempts += 1\n",
    "                if attempts == 3:\n",
    "                    continue\n",
    "\n",
    "        # try:\n",
    "        #\n",
    "        #     response = urllib.request.urlopen(request,timeout=3.5) #访问合约页面\n",
    "        # except:\n",
    "        #     print(\"pass_\"+url)\n",
    "        #     pass\n",
    "        #     continue\n",
    "        contract = response.read().decode(\"utf-8\")  #解析合约页面\n",
    "        response.close()\n",
    "\n",
    "        ds = BeautifulSoup(contract, \"html.parser\")#用html解析打开\n",
    "        contract = ds.find_all(class_=\"js-sourcecopyarea editor\") #定位页面中的合约信息\n",
    "        print(\"contract:\" + url)\n",
    "        if len(contract)!=0:\n",
    "            text=contract[0]#只取合约 去除标签\n",
    "            result=text.get_text()# 转成string返回给result  因为write只能写string\n",
    "            filename = 'D:\\\\workandlearning\\\\LLM\\\\smart_contract\\\\contract_'+str(count) + '_sourceCode.sol'\n",
    "            count=count+1\n",
    "            \n",
    "\n",
    "            with open(filename, 'w',encoding='utf-8') as file_object:\n",
    "                 file_object.write(str(result))\n",
    "\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c2aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafcd4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c5e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "askURL()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
